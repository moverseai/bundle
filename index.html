<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse Multiview Videos">
  <meta name="keywords" content="Moverse, AI, Motion Capture, MoCap, SMPL, SMPL-X, Body Fitting, Body Estimation, Pose Estimation, Multiview System">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- <title>MoCatalyst: Accelerating and Automating MoCap</title> -->
  <title>BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse Multiview Videos</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXXX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-XXXXXXXXXXXXX');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="stylesheet" href="./static/css/dics.css">
  <script src="./static/js/dics.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://www.github.com/moverseai">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://moverseai.github.io/noise-tail">
            Long-tail
          </a>
          <a class="navbar-item" href="https://moverseai.github.io/bundle">
            BundleMoCap
          </a>
          <!-- <a class="navbar-item" href="https://moverseai.github.io/Placeholder3">
            Moverse Placeholder #3
          </a>
          <a class="navbar-item" href="https://moverseai.github.io/Placeholder4">
            Moverse Placeholder #4
          </a> -->
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img width=8% src="./static/images/mov_icon.png" class="center">
          <br><br>
          <h1 class="title is-1 publication-title">BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse Multiview Videos</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a href="https://tzole1155.github.io/">Georgos Albanis</a><sup>1, 2</sup>,</span>
            <span class="author-block">
                <a href="https://zokin.github.io">Nikolaos Zioulis</a><sup>1</sup>,</span>
            <span class="author-block">
                <a href="http://kostasks.users.uth.gr/index.html">Kostas Kolomvatsos</a><sup>2</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup><a href="https://www.moverse.ai">Moverse</a>,</span>
            <span class="author-block"><sup>2</sup><a href="https://iprism.eu/">University of Thessaly</a></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/ICCV2023W/WORKSHOP/papers/XXXX_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/ICCV2023W/WORKSHOP/supplemental/XXXX_supplemental.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span>  
              <span class="link-block">
                <a href="https://arxiv.org/pdf/23XX.XXXXX.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>                                                                    -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://youtu.be/X9QLlEbKKnQ?si=mMhfhY_zcsPsrH65"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/moverseai/noise-tail"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/moverseai/noise-tail"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container has-text-centered is-max-desktop">    
      <img width=80% src="./static/images/teaser.png" class="center">
  </div>
</section>


<!-- Abstract. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Capturing smooth motions from videos using markerless techniques typically involves complex processes such as temporal constraints, multiple stages with data-driven regression and optimization, and bundle solving over temporal windows.
            These processes can be inefficient and require tuning multiple objectives across stages.
            In contrast, <span class="ours">BundleMoCap</span> introduces a novel and efficient approach to this problem. It solves the motion capture task in a single stage, eliminating the need for temporal smoothness objectives while still delivering smooth motions. 
            <span class="ours">BundleMoCap</span> outperforms the state-of-the-art without increasing complexity.
            The key concept behind <span class="ours">BundleMoCap</span> is manifold interpolation between latent keyframes. 
            By relying on a local manifold smoothness assumption, we can efficiently solve a bundle of frames using a single code. 
            Additionally, the method can be implemented as a sliding window optimization and requires only the first frame to be properly initialized, reducing the overall computational burden.
            <span class="ours">BundleMoCap's</span> strength lies in its ability to achieve high-quality motion capture results with simplicity and efficiency.
          </p>
        </div>
      </div>
    </div>
<!--/ Abstract. -->

<!-- Flow. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-centered has-text-centered">
        <h3 class="title is-3">Overview</h3>
        <!-- <div class="content has-text-justified">
        </div> -->
        <div class="container has-text-centered is-max-desktop">    
            <img width=100% src="./static/images/bundle_mocap.png" class="center">
        </div>
        <p>
          <span class="ours">BundleMoCap</span> fits an articulated template mesh to 2D keypoint observations from a sparse set of multi-view videos.          
          Instead of iteratively optimizing pose parameters for each frame, we focus on optimizing the latent code \(z^t\) corresponding to the pose parameters \(\theta^t = \mathcal{G}(z^t)\) for a single keyframe (\(t^i=T\)).           
          This relies on the reconstruction of the poses, root orientation and translation via interpolation, generating the intermediate frames (visually indicated by the blending between the start and end keyframes). 
          A sliding window optimization implementation is used where only the first frame is fit in a standalone manner.           
          Then, the \(i\text{th}\) temporal window \(\mathcal{T}^i\) that is solved as a bundle, optimizes only the next latent keyframe (\(t^i=T\)), while reconstructing the frames between the previously optimized keyframe (\(t^i=0\)) and the next.
          All reconstructed frames are constrained by the respective multi-view keypoint constraints via \(\mathcal{E}^\mathcal{T}_{data}\) while the latent keyframe is regularized by \(\mathcal{E}^\mathcal{T}_{prior}\).
          <span class="ours">BundleMoCap</span> requires just a single stage to achieve comparable results with the state-of-the-art without pose initialisation for each frame and delivers smooth motions efficiently without using any motion smoothness objective.
        </p>
      </div>
    </div>
  </div>
</section>
<!--/ Flow. -->

<!-- Results. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-centered has-text-centered">
        <h3 class="title is-3">Results</h3>
        <div>
          <iframe src="./static/rerun/results_1.html" width="1000px" height="900px"></iframe>
      </div>
    </div>
  </div>
  <div class="sketchfab-embed-wrapper">
    <iframe title="Mpi_bundle" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share src="https://sketchfab.com/models/c7cf89253cf34fc7bdd4b06b951fe24d/embed">
    </iframe> <p style="font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;"> 
      <!-- <a href="https://sketchfab.com/3d-models/mpi-bundle-c7cf89253cf34fc7bdd4b06b951fe24d?utm_medium=embed&utm_campaign=share-popup&utm_content=c7cf89253cf34fc7bdd4b06b951fe24d" target="_blank" rel="nofollow" style="font-weight: bold; color: #1CAAD9;"> Bundle </a> by <a href="https://sketchfab.com/giorgos_al10?utm_medium=embed&utm_campaign=share-popup&utm_content=c7cf89253cf34fc7bdd4b06b951fe24d" target="_blank" rel="nofollow" style="font-weight: bold; color: #1CAAD9;"> giorgos_al10 </a> on <a href="https://sketchfab.com?utm_medium=embed&utm_campaign=share-popup&utm_content=c7cf89253cf34fc7bdd4b06b951fe24d" target="_blank" rel="nofollow" style="font-weight: bold; color: #1CAAD9;">Sketchfab</a> -->
    </p>
  </div>
</section>
<!--/ Results. -->

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- <img width=8% src="./static/images/mov_icon.png" class="center"> -->
          <br><br>
          <h1 class="title is-1 publication-title">MoCatalyst: Accelerating and Automating MoCap</h1>
          <h4 class="title is-5 publication-title"><it>[Demonstration of BundleMoCap as a neural inverse kinematics solver to 3D landmark constraints]</it></h4>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a href="https://tzole1155.github.io/">Georgos Albanis</a><sup>1, 2</sup>,</span>
            <span class="author-block">
                <a href="https://zokin.github.io">Nikolaos Zioulis</a><sup>1</sup>,</span>
            <span class="author-block">
                <a href="https://spthermo.github.io/">Spyridon Thermos</a><sup>1</sup>,</span>
            <span class="author-block">
                <a href="https://tofis.github.io/">Chatzitofis Anargyros</a><sup>1</sup>,</span>
            <span class="author-block">
                <a href="http://kostasks.users.uth.gr/index.html">Kostas Kolomvatsos</a><sup>2</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup><a href="https://www.moverse.ai">Moverse</a>,</span>
            <span class="author-block"><sup>2</sup><a href="https://iprism.eu/">University of Thessaly</a></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/ICCV2023W/WORKSHOP/papers/XXXX_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/ICCV2023W/WORKSHOP/supplemental/XXXX_supplemental.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span>  
              <span class="link-block">
                <a href="https://arxiv.org/pdf/23XX.XXXXX.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>                                                                    -->
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/X9QLlEbKKnQ?si=mMhfhY_zcsPsrH65"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/moverseai/noise-tail"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/moverseai/noise-tail"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Motivation. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Raw MoCap data need post-processing even when captured with high-end systems.
            This process usually involves one or more animators, it is tedious and time-consuming.
            Further, the advent of AI-powered MoCap enables capturing with more generic and lower-cost cameras,
            introducing additional noise to the captured data, making the need for automatic post-processing
            more imminent. 
          </p>
        </div>
      </div>
    </div>
<!--/ Motivation. -->

<!-- Paper video. -->
<section>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Video</h2> -->
        <div class="publication-video">
          <iframe width="1280" height="720" src="https://www.youtube.com/embed/X9QLlEbKKnQ" title="Moverse @ICCV2023  | Demo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </div>
</section>
<!--/ Paper video. -->


<!-- Flow. -->
<section class="section">
  <div class="container is-max-desktop">
      <div class="column is-centered has-text-centered">
        <h2 class="title is-3">Demo Summary</h2>
      <div class="content has-text-justified">
        <p>
          We present a novel solution that alleviates the issues associated with the arduous
          post-processing of raw motion data, involving the rectification of errors such as
          missing, mislabeled, or occluded markers. Such challenges are exacerbated when
          utilizing low-cost sensing devices, where such errors are amplified. To that end,
          we introduce a real-time and artefact-free MoCap-solving data-driven model, which
          combined with an innovative noise-aware temporal fitting method, enables high-quality
          MoCap even when using a sparse set of low-cost sensors. Our fitting method can process
          any raw motion data, regardless of the capturing method, including
          both optical and inertial. This means it is not limited to specific sensor types and
          can handle a wide range of data sources. Our data-driven model can simultaneously 
          denoise, solve, and hallucinate the raw unstructured point cloud, while our fitting 
          approach models the uncertainty region of measurements and refines the real-time
          MoCap data by optimizing within a temporal window. This results in more accurate 
          outcomes and ensures temporal coherence, preventing common failures and induced artefacts.
        </p>
      </div>
        <h2 class="title is-3">Demo Setup</h2>
        <div class="content has-text-justified">
          <p>
            The demo consists of 4 distinct step, although highlighting the automatic
            post-processing technology for smoothing and refining the captured data: a)
            capture the user's movement with camera sensors, b) estimate user's body pose
            in real-time (check <a href="https://moverseai.github.io/noise-tail/">here</a> how),
            c) refine the initial estimation by fitting an <a href="https://smpl.is.tue.mpg.de/">articulated template mesh</a> to
            3D landmarks, and d) Visualize the comparison between the fitted meshes of the real-time vs postprocessed
            results, their 3D skeleton reprojections, and joints angles.
          </p>
        </div>
        <div class="container has-text-centered is-max-desktop">    
            <img width=100% src="./static/images/demo-flow.png" class="center">
        </div>
      </div>
  </div>
</section>
<!--/ Flow. -->

<!-- Flow. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-centered has-text-centered">
        <h3 class="title is-3">Where To Find Us!</h3>
        <!-- <div class="content has-text-justified">
        </div> -->
        <div class="container has-text-centered is-max-desktop">    
            <img width=100% src="./static/images/demo-program.png" class="center">
        </div>
      </div>
    </div>
  </div>
</section>
<!--/ Flow. -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{albanis2023bundle,
      author    = {Albanis, Georgios, and Zioulis, Nikolaos, and Kolomvatsos, Kostas.},
      title     = {BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse Multiview Videos},
      booktitle = {20th ACM SIGGRAPH European Conference on Visual Media Production (CVMP) 2023},
      url       = {https://moverseai.github.io/bundle/},
      month     = {December},
      year      = {2023}  
    }</code></pre>
    <pre><code>@inproceedings{albanis2023bundle,
      author    = {Albanis, Georgios, and Zioulis, Nikolaos, and Thermos, Spyridon, and Chatzitofis, Anargyros and Kolomvatsos, Kostas.},
      title     = {MoCatalyst: Accelerating and Automating MoCap},
      booktitle = {IEEE/CVF International Conference on Computer Vision (ICCV) Demo},
      url       = {https://moverseai.github.io/bundle/},
      month     = {October},
      year      = {2023}  
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/pdf/paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <!-- <a class="icon-link" href="https://github.com/moverseai" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="columns is-centered">
        <div class="content">
          <p>
            The website template is borrowed from <a href="https://nerfies.github.io" target="_blank">nerfies</a>.
          </p>
        </div>
    </div>
  </div>
</footer>

</body>
</html>