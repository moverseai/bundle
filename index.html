<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse Multiview Videos">
  <meta name="keywords" content="Moverse, AI, Motion Capture, MoCap, SMPL, SMPL-X, Body Fitting, Body Estimation, Pose Estimation, Multiview System">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse Multiview Videos</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXXX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-XXXXXXXXXXXXX');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="stylesheet" href="./static/css/dics.css">
  <script src="./static/js/dics.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://www.github.com/moverseai">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://moverseai.github.io/noise-tail">
            Long-tail
          </a>
          <a class="navbar-item" href="https://moverseai.github.io/bundle">
            BundleMoCap
          </a>
          <!-- <a class="navbar-item" href="https://moverseai.github.io/Placeholder3">
            Moverse Placeholder #3
          </a>
          <a class="navbar-item" href="https://moverseai.github.io/Placeholder4">
            Moverse Placeholder #4
          </a> -->
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img width=8% src="./static/images/mov_icon.png" class="center">
          <br><br>
          <h1 class="title is-1 publication-title">BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse Multiview Videos</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a href="https://tzole1155.github.io/">Georgios Albanis</a><sup>1, 2</sup>,</span>
            <span class="author-block">
                <a href="https://zokin.github.io">Nikolaos Zioulis</a><sup>1</sup>,</span>
            <span class="author-block">
                <a href="http://kostasks.users.uth.gr/index.html">Kostas Kolomvatsos</a><sup>2</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup><a href="https://www.moverse.ai">Moverse</a>,</span>
            <span class="author-block"><sup>2</sup><a href="https://iprism.eu/">University of Thessaly</a></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2311.12679.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> 
              <!-- <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/ICCV2023W/WORKSHOP/papers/XXXX_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/ICCV2023W/WORKSHOP/supplemental/XXXX_supplemental.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span>  
              <span class="link-block">
                <a href="https://arxiv.org/pdf/23XX.XXXXX.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>                                                                    -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://youtu.be/X9QLlEbKKnQ?si=mMhfhY_zcsPsrH65"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/moverseai/noise-tail"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/moverseai/noise-tail"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container has-text-centered is-max-desktop">    
      <img width=80% src="./static/images/teaser.png" class="center">
  </div>
</section>


<!-- Abstract. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Capturing smooth motions from videos using markerless techniques typically involves complex processes such as temporal constraints, multiple stages with data-driven regression and optimization, and bundle solving over temporal windows.
            These processes can be inefficient and require tuning multiple objectives across stages.
            In contrast, <span class="ours">BundleMoCap</span> introduces a novel and efficient approach to this problem. It solves the motion capture task in a single stage, eliminating the need for temporal smoothness objectives while still delivering smooth motions. 
            <span class="ours">BundleMoCap</span> outperforms the state-of-the-art without increasing complexity.
            The key concept behind <span class="ours">BundleMoCap</span> is manifold interpolation between latent keyframes. 
            By relying on a local manifold smoothness assumption, we can efficiently solve a bundle of frames using a single code. 
            Additionally, the method can be implemented as a sliding window optimization and requires only the first frame to be properly initialized, reducing the overall computational burden.
            <span class="ours">BundleMoCap's</span> strength lies in its ability to achieve high-quality motion capture results with simplicity and efficiency.
          </p>
        </div>
      </div>
    </div>
<!--/ Abstract. -->

<!-- Flow. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-centered has-text-centered">
        <h3 class="title is-3">Overview</h3>
        <!-- <div class="content has-text-justified">
        </div> -->
        <div class="container has-text-centered is-max-desktop">    
            <img width=100% src="./static/images/bundle_mocap.png" class="center">
        </div>
        <p>
          <span class="ours">BundleMoCap</span> fits an articulated template mesh to 2D keypoint observations from a sparse set of multi-view videos.          
          Instead of iteratively optimizing pose parameters for each frame, we focus on optimizing the latent code \(z^t\) corresponding to the pose parameters \(\theta^t = \mathcal{G}(z^t)\) for a single keyframe (\(t^i=T\)).           
          This relies on the reconstruction of the poses, root orientation and translation via interpolation, generating the intermediate frames (visually indicated by the blending between the start and end keyframes). 
          A sliding window optimization implementation is used where only the first frame is fit in a standalone manner.           
          Then, the \(i\text{th}\) temporal window \(\mathcal{T}^i\) that is solved as a bundle, optimizes only the next latent keyframe (\(t^i=T\)), while reconstructing the frames between the previously optimized keyframe (\(t^i=0\)) and the next.
          All reconstructed frames are constrained by the respective multi-view keypoint constraints via \(\mathcal{E}^\mathcal{T}_{data}\) while the latent keyframe is regularized by \(\mathcal{E}^\mathcal{T}_{prior}\).
          <span class="ours">BundleMoCap</span> requires just a single stage to achieve comparable results with the state-of-the-art without pose initialisation for each frame and delivers smooth motions efficiently without using any motion smoothness objective.
        </p>
      </div>
    </div>
  </div>
</section>
<!--/ Flow. -->

<!-- Results. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-centered has-text-centered">
        <h3 class="title is-3">Results</h3>
        <!-- <div>
          <iframe src="./static/rerun/results_1.html" width="1000px" height="900px"></iframe>
      </div> -->
      <h4 class="title is-3">MPI-INF-3DHP</h4>
      <div class="content has-text-justified">
        <p>
          <span class="ours">BundleMoCap</span> on the <a href="https://paperswithcode.com/dataset/mpi-inf-3dhp">MPI</a> dataset, subject <b>S8</b>, and <b>Sequence 2</b>, on different time windows.
        </p>
      </div>
      <div class="columns is-full-width">  
        <div class="sketchfab-embed-wrapper">
          <iframe title="Mpi_bundle" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width="640" height="480" src="https://sketchfab.com/models/c7cf89253cf34fc7bdd4b06b951fe24d/embed"></iframe>
        </div>
        <div class="sketchfab-embed-wrapper">
          <iframe title="Mpi_bundle v2" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width="640" height="480" src="https://sketchfab.com/models/c77a55492c7f4d67b3dffeb7e30f431e/embed"></iframe>
        </div>
      </div>
      <h4 class="title is-3">In the wild</h4>
      <div class="content has-text-justified">
        <p>
          <span class="ours">BundleMoCap</span> results on fast dynamic motion using a 7 low-cost camera capture system.
        </p>
      </div>
      <div class="columns is-full-width">  
          <div class="sketchfab-embed-wrapper">
              <iframe title="Kick" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share" width="640" height="480" src="https://sketchfab.com/models/b9b68c3363684e8986cb2b38cd6d2cab/embed"></iframe>
          </div>
          <div class="sketchfab-embed-wrapper">
              <iframe title="Fighting" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share" width="640" height="480" src="https://sketchfab.com/models/45c4ef345a724466af64e16b28847790/embed"></iframe>
          </div>
      </div>
    </div>
  </div>

</section>
<!--/ Results. -->
<!-- Comparison. -->
<section class="section">
  <div class="container is-max-desktop">
      <!-- columns is-full-width -->
      <div class="columns is-centered">
        <div class="column is-centered has-text-centered">
          <h3 class="title is-3">Comparison</h3>
          <p>
          <b>Comparison with different architectures.</b>
            This video presents a comparative analysis of various multi-view methods on <a href="http://vision.imar.ro/human3.6m/description.php">Human3.6M</a> dataset on various actions.
            Our study begins with a standard multi-view fitting as a baseline <a href="#MuVS">MuVS <b>[1]</b></a>, then advances to explore more sophisticated techniques such as
            <a href="#DCT">DCT <b>[2]</b></a>, 
            <a href="#ETC">ETC <b>[3]</b></a>, 
            <a href="#DMMR">DMMR <b>[4]</b></a>, 
            and <a href="#SLAHMR">SLAHMR <b>[5]</b></a>.
             <!-- integrate data-driven priors like <a href="#VPoser">VPoser <b>[6]</b></a> and <a href="#HuMoR">HuMoR <b>[7]</b></a> to refine motion capture precision beyond traditional methods. -->
          </p>
          <video muted="" loop="" playsinline="" controls="" width="100%">
            <source src="./static/videos/sitting_down.mp4" type="video/mp4">
        </video>
        <video muted="" loop="" playsinline="" controls="" width="100%">
          <source src="./static/videos/phoning.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
  <div class="container is-max-desktop">
    <!-- columns is-full-width -->
    <div class="columns is-centered">
      <div class="column is-centered has-text-centered">
        <h3 class="title is-3">Perfomance vs Efficiency</h3>
        <p>
        A visual summary of the performance (horizontal and vertices axes) and efficiency (runtime illustrated as the size of each point) of different methods. 
        BundleMoCap’s competitive results are achieved with a minimal computational burden, without the need for 3D initialization, or a smoothness objective. 
        Its efficiency is greatly boosted from its single-stage nature, making it an more appropriate choice for practical applications.
        <img width=50% src="./static/images/efficiency.png" class="center">
    </div>
  </div>
</div>
</section>
<!--/ Comparison. -->
<!-- BibTeX. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-centered has-text-centered">
        <h2 class="title is-3">References</h2>
        <ol>
          <li>
            <a href="https://smpl-x.is.tue.mpg.de/" id="MuVS">
            Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas and Michael J. Black. Expressive Body Capture: 3D Hands, Face, and Body from a Single Image. In 2019 Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR).
          </a>
          </li>
          <li>
            <a href="https://ieeexplore.ieee.org/document/8374596" id="DCT">
            Yinghao Huang, Federica Bogo, Christoph Lassner, Angjoo Kanazawa, Peter V Gehler,
            Javier Romero, Ijaz Akhter, and Michael J Black. 2017. Towards accurate markerless
            human shape and pose estimation over time. In 2017 international conference on 3D
            vision (3DV). IEEE, 421–430.
          </a>
          </li>
          <li>
            <a href="https://arxiv.org/abs/1905.04266" id="ETC">
              Anurag Arnab, Carl Doersch, and Andrew Zisserman. 2019. Exploiting temporal
              context for 3D human pose estimation in the wild. In Proceedings of the IEEE/CVF
              Conference on Computer Vision and Pattern Recognition. 3395–3404
            </a>
          </li>
          <li>
            <a href="https://arxiv.org/abs/2110.10355" id="DMMR">
              Buzhen Huang, Yuan Shu, Tianshu Zhang, and Yangang Wang. 2021. Dynamic multiperson mesh recovery from uncalibrated multi-view cameras. In 2021 International
              Conference on 3D Vision (3DV). IEEE, 710–720.
            </a>
          </li>
          <li>
            <a href="https://arxiv.org/abs/2302.12827" id="SLAHMR">
              Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. 2023. Decoupling
              human and camera motion from videos in the wild. In Proceedings of the IEEE/CVF
              Conference on Computer Vision and Pattern Recognition. 21222–21232.
            </a>
          </li>
          <!-- <li id="VPoser">[Reference for VPoser]</li>
          <li id="HuMoR">[Reference for HuMoR]</li> -->
        </ol>
      </div>
    </div>
  </div>
</section>
<!--/ BibTeX. -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- <img width=8% src="./static/images/mov_icon.png" class="center"> -->
          <br><br>
          <h1 class="title is-1 publication-title">MoCatalyst: Accelerating and Automating MoCap</h1>
          <h4 class="title is-5 publication-title"><it>[Demonstration of BundleMoCap as a neural inverse kinematics solver to 3D landmark constraints]</it></h4>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a href="https://tzole1155.github.io/">Georgios Albanis</a><sup>1, 2</sup>,</span>
            <span class="author-block">
                <a href="https://zokin.github.io">Nikolaos Zioulis</a><sup>1</sup>,</span>
            <span class="author-block">
                <a href="https://spthermo.github.io/">Spyridon Thermos</a><sup>1</sup>,</span>
            <span class="author-block">
                <a href="https://tofis.github.io/">Chatzitofis Anargyros</a><sup>1</sup>,</span>
            <span class="author-block">
                <a href="http://kostasks.users.uth.gr/index.html">Kostas Kolomvatsos</a><sup>2</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup><a href="https://www.moverse.ai">Moverse</a>,</span>
            <span class="author-block"><sup>2</sup><a href="https://iprism.eu/">University of Thessaly</a></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/ICCV2023W/WORKSHOP/papers/XXXX_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/ICCV2023W/WORKSHOP/supplemental/XXXX_supplemental.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span>  
              <span class="link-block">
                <a href="https://arxiv.org/pdf/23XX.XXXXX.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>                                                                    -->
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/X9QLlEbKKnQ?si=mMhfhY_zcsPsrH65"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/moverseai/noise-tail"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/moverseai/noise-tail"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Motivation. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Raw MoCap data need post-processing even when captured with high-end systems.
            This process usually involves one or more animators, it is tedious and time-consuming.
            Further, the advent of AI-powered MoCap enables capturing with more generic and lower-cost cameras,
            introducing additional noise to the captured data, making the need for automatic post-processing
            more imminent. 
          </p>
        </div>
      </div>
    </div>
<!--/ Motivation. -->

<!-- Paper video. -->
<section>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Video</h2> -->
        <div class="publication-video">
          <iframe width="1280" height="720" src="https://www.youtube.com/embed/X9QLlEbKKnQ" title="Moverse @ICCV2023  | Demo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </div>
</section>
<!--/ Paper video. -->


<!-- Flow. -->
<section class="section">
  <div class="container is-max-desktop">
      <div class="column is-centered has-text-centered">
        <h2 class="title is-3">Demo Summary</h2>
      <div class="content has-text-justified">
        <p>
          We present a novel solution that alleviates the issues associated with the arduous
          post-processing of raw motion data, involving the rectification of errors such as
          missing, mislabeled, or occluded markers. Such challenges are exacerbated when
          utilizing low-cost sensing devices, where such errors are amplified. To that end,
          we introduce a real-time and artefact-free MoCap-solving data-driven model, which
          combined with an innovative noise-aware temporal fitting method, enables high-quality
          MoCap even when using a sparse set of low-cost sensors. Our fitting method can process
          any raw motion data, regardless of the capturing method, including
          both optical and inertial. This means it is not limited to specific sensor types and
          can handle a wide range of data sources. Our data-driven model can simultaneously 
          denoise, solve, and hallucinate the raw unstructured point cloud, while our fitting 
          approach models the uncertainty region of measurements and refines the real-time
          MoCap data by optimizing within a temporal window. This results in more accurate 
          outcomes and ensures temporal coherence, preventing common failures and induced artefacts.
        </p>
      </div>
        <h2 class="title is-3">Demo Setup</h2>
        <div class="content has-text-justified">
          <p>
            The demo consists of 4 distinct step, although highlighting the automatic
            post-processing technology for smoothing and refining the captured data: a)
            capture the user's movement with camera sensors, b) estimate user's body pose
            in real-time (check <a href="https://moverseai.github.io/noise-tail/">here</a> how),
            c) refine the initial estimation by fitting an <a href="https://smpl.is.tue.mpg.de/">articulated template mesh</a> to
            3D landmarks, and d) Visualize the comparison between the fitted meshes of the real-time vs postprocessed
            results, their 3D skeleton reprojections, and joints angles.
          </p>
        </div>
        <div class="container has-text-centered is-max-desktop">    
            <img width=100% src="./static/images/demo-flow.png" class="center">
        </div>
      </div>
  </div>
</section>
<!--/ Flow. -->

<!-- Flow. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-centered has-text-centered">
        <h3 class="title is-3">Where To Find Us!</h3>
        <!-- <div class="content has-text-justified">
        </div> -->
        <div class="container has-text-centered is-max-desktop">    
            <img width=100% src="./static/images/demo-program.png" class="center">
        </div>
      </div>
    </div>
  </div>
</section>
<!--/ Flow. -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{albanis2023bundle,
      author    = {Albanis, Georgios, and Zioulis, Nikolaos, and Kolomvatsos, Kostas.},
      title     = {BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse Multiview Videos},
      booktitle = {20th ACM SIGGRAPH European Conference on Visual Media Production (CVMP) 2023},
      url       = {https://moverseai.github.io/bundle/},
      month     = {December},
      year      = {2023}  
    }</code></pre>
    <pre><code>@inproceedings{albanis2023bundle,
      author    = {Albanis, Georgios, and Zioulis, Nikolaos, and Thermos, Spyridon, and Chatzitofis, Anargyros and Kolomvatsos, Kostas.},
      title     = {MoCatalyst: Accelerating and Automating MoCap},
      booktitle = {IEEE/CVF International Conference on Computer Vision (ICCV) Demo},
      url       = {https://moverseai.github.io/bundle/},
      month     = {October},
      year      = {2023}  
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/pdf/paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <!-- <a class="icon-link" href="https://github.com/moverseai" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="columns is-centered">
        <div class="content">
          <p>
            The website template is borrowed from <a href="https://nerfies.github.io" target="_blank">nerfies</a>.
          </p>
        </div>
    </div>
  </div>
</footer>

</body>
</html>